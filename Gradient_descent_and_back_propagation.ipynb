{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatasl/AIML_TRAINING_VENKAT/blob/main/Gradient_descent_and_back_propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "215d9cd7",
      "metadata": {
        "id": "215d9cd7"
      },
      "source": [
        "# Gradient Descent Tutorial\n",
        "\n",
        "Link to Presentation: https://docs.google.com/presentation/d/1basYOmB3uW8l-p-v1OChzzYI0bzm-r3dr8wFTRpluhk/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34a1283",
      "metadata": {
        "id": "a34a1283"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a8a5d1c3",
      "metadata": {
        "id": "a8a5d1c3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55dffef2",
      "metadata": {
        "id": "55dffef2"
      },
      "source": [
        "## Calculating the gradients of functions\n",
        "\n",
        "Let us start with a simple function: sum of squares. Its gradient is known as 2*x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a6bb8397",
      "metadata": {
        "id": "a6bb8397"
      },
      "outputs": [],
      "source": [
        "def f1(X):\n",
        "    return torch.sum(torch.square(X))\n",
        "\n",
        "\n",
        "def grad_f1(X):\n",
        "    grad = torch.zeros_like(X)\n",
        "    for i in range(len(X)):\n",
        "        grad[i] = 2*X[i]\n",
        "    return grad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us see what our function does"
      ],
      "metadata": {
        "id": "eqU6-KR-UNr3"
      },
      "id": "eqU6-KR-UNr3"
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1,2,3]);\n",
        "print('Original X: ', x)\n",
        "print('f1(x): ',f1(x))\n",
        "print('Calculated d(f1)/d(x): ',grad_f1(x))"
      ],
      "metadata": {
        "id": "4KnkWduDPvJ8",
        "outputId": "2ce0bd67-794f-48ca-d83f-4ffcdfd9261f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4KnkWduDPvJ8",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original X:  tensor([1, 2, 3])\n",
            "f1(x):  tensor(14)\n",
            "Calculated d(f1)/d(x):  tensor([2, 4, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't need to calculate the gradient function by ourselves. Pytorch provides **autograd** functionality which calculates the gradients for us on the fly. To do that, we need to define a Pytorch tensor with the flag **requires_grad=True**. We also need to make sure all the functions we are applying on the tensor is done using pytorch functions.\n",
        "\n",
        "For all tensors with requires_grad=True, Pytorch maintains a graph of the operations so that the gradient can be calculated when required, using **backward** function."
      ],
      "metadata": {
        "id": "_LD3WZB4UZEq"
      },
      "id": "_LD3WZB4UZEq"
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([4,5,6], dtype=float, requires_grad=True);\n",
        "print('Original X: ', x)\n",
        "y = f1(x)\n",
        "print('f1(x): ', y)\n",
        "dy = grad_f1(x)\n",
        "print('Calculated gradient dy/dx: ', dy.data)\n",
        "# calculate the gradients using autograd\n",
        "y.backward()\n",
        "# now dy/dx will be present in x.grad\n",
        "print('Autograd dy/dx: ', x.grad)"
      ],
      "metadata": {
        "id": "Bgf0YbpSRcn8",
        "outputId": "14372702-39df-47d4-961b-637d1d680c35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Bgf0YbpSRcn8",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original X:  tensor([4., 5., 6.], dtype=torch.float64, requires_grad=True)\n",
            "f1(x):  tensor(77., dtype=torch.float64, grad_fn=<SumBackward0>)\n",
            "Calculated gradient dy/dx:  tensor([ 8., 10., 12.], dtype=torch.float64)\n",
            "Autograd dy/dx:  tensor([ 8., 10., 12.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try a more complex function with multiple steps:\n",
        "z = (x^2 + y)^3\n",
        "\n",
        "Let us calculate by hand. Let us break it like this:\n",
        "w = x^2 + y\n",
        "z = w^3\n",
        "\n",
        "dz/dw = 3w^2\n",
        "dw/dx = 2x\n",
        "dw/dy = 1\n",
        "dz/dx = dz/dw*dw/dx  etc."
      ],
      "metadata": {
        "id": "lMBKgHSbVJx8"
      },
      "id": "lMBKgHSbVJx8"
    },
    {
      "cell_type": "code",
      "source": [
        "# input\n",
        "x = torch.tensor([2], dtype=float, requires_grad=True)\n",
        "print('original x: ', x)\n",
        "y = torch.tensor([5], dtype=float, requires_grad=True)\n",
        "print('original y: ', y)\n",
        "\n",
        "# calculate our function\n",
        "def f2(x,y):\n",
        "  w = x*x + y\n",
        "  z = w**3\n",
        "  return w, z\n",
        "\n",
        "w,z = f2(x,y)\n",
        "print('intermediate w: ', w.data)\n",
        "print('z: ', z.data)"
      ],
      "metadata": {
        "id": "BQS3i74yXKcQ"
      },
      "id": "BQS3i74yXKcQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us check the gradients computed by Pytorch for the w function:"
      ],
      "metadata": {
        "id": "yC67gdy_YKeB"
      },
      "id": "yC67gdy_YKeB"
    },
    {
      "cell_type": "code",
      "source": [
        "w.backward()\n",
        "print('dw/dx : ', x.grad)\n",
        "print('dw/dy : ', y.grad)"
      ],
      "metadata": {
        "id": "spFvFtP6YYfG"
      },
      "id": "spFvFtP6YYfG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us calculate the gradients for the z function:"
      ],
      "metadata": {
        "id": "FVokjhNBZxfM"
      },
      "id": "FVokjhNBZxfM"
    },
    {
      "cell_type": "code",
      "source": [
        "z.backward()"
      ],
      "metadata": {
        "id": "FzsNqyzQZo2c"
      },
      "id": "FzsNqyzQZo2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Pytorch, we can't run **backward()** through the same graph twice to save memory. So we have to run the function again or put the flag **retain_graph=True**"
      ],
      "metadata": {
        "id": "-kxBl6VkZ1EM"
      },
      "id": "-kxBl6VkZ1EM"
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to clear all previous gradients, otherwise, it will get added up\n",
        "x = x.detach()\n",
        "y = y.detach()\n",
        "x.requires_grad=True\n",
        "y.requires_grad=True\n",
        "\n",
        "\n",
        "w,z = f2(x,y)\n",
        "\n",
        "z.backward()\n",
        "\n",
        "print('dz/dx = dz/dw * dw/dx: ', x.grad)\n",
        "print('dz/dy = dz/dw * dw/dy: ', y.grad)"
      ],
      "metadata": {
        "id": "ixUiJi84aFtg"
      },
      "id": "ixUiJi84aFtg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "* Try with your own functions of multiple steps and multiple variables. Does the calculated gradient match the actual gradient?\n",
        "* How do we calculate the gradient of a vector?"
      ],
      "metadata": {
        "id": "1a5f1JrskdZ_"
      },
      "id": "1a5f1JrskdZ_"
    },
    {
      "cell_type": "markdown",
      "id": "e80cf9d9",
      "metadata": {
        "id": "e80cf9d9"
      },
      "source": [
        "# Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try to find the minimum of a simple function x^2. First let us look at the graph of the function."
      ],
      "metadata": {
        "id": "M4Yw8WQleaTW"
      },
      "id": "M4Yw8WQleaTW"
    },
    {
      "cell_type": "code",
      "source": [
        "def f3(x):\n",
        "  return x**2\n",
        "\n",
        "def plot_function(function, start, end):\n",
        "  x = torch.linspace(start, end, 1000)\n",
        "  y = f3(x)\n",
        "  plt.plot(x,y)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GkdGX9aSeTfw"
      },
      "id": "GkdGX9aSeTfw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_function(f3, -1000,1000)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yw2MtBZ7fX5w"
      },
      "id": "yw2MtBZ7fX5w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a function for **Gradient Descent**, where we start with a random **x**, and move slowly in the negative gradient direction"
      ],
      "metadata": {
        "id": "WNPAs67Ef41U"
      },
      "id": "WNPAs67Ef41U"
    },
    {
      "cell_type": "markdown",
      "id": "6e3366e0",
      "metadata": {
        "id": "6e3366e0"
      },
      "source": [
        "**Note:** Talk about convergence_threshold\n",
        "- Just a bargain between time and precision\n",
        "- Near the minima, update rate becomes very slow\n",
        "- Also, getting an exact match for complex functions is difficult (splly when going stochastic)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_visualize(function, lr=0.1, convergence_threshold = 0.01, max_iter = 10000):\n",
        "  x = torch.rand(1, requires_grad=True)\n",
        "  x_history = []\n",
        "  for ii in range(max_iter):\n",
        "    y = function(x)\n",
        "    y.backward()\n",
        "    x_new = x - x.grad*lr\n",
        "    if torch.abs(x_new - x) < convergence_threshold:\n",
        "      x = x_new\n",
        "      break\n",
        "    # store the history of steps so that we can see\n",
        "    x_history = x_history + [x_new]\n",
        "    # zero the gradients\n",
        "    x = x_new.detach()\n",
        "    x.requires_grad = True\n",
        "\n",
        "  return x, torch.tensor(x_history)\n"
      ],
      "metadata": {
        "id": "2KthBxurgFza"
      },
      "id": "2KthBxurgFza",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, history = gradient_descent_visualize(f3 , lr=0.1)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "lek8d9QKgR6y"
      },
      "id": "lek8d9QKgR6y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us write a function for visualizing the steps taken"
      ],
      "metadata": {
        "id": "L-LZ3MashfLN"
      },
      "id": "L-LZ3MashfLN"
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_function_2(function, history):\n",
        "  y = function(history)\n",
        "  plt.plot(history, y,color='red', marker='o', fillstyle='full')"
      ],
      "metadata": {
        "id": "F3GtAB0JhoAa"
      },
      "id": "F3GtAB0JhoAa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_function(f3, -1, 1)\n",
        "plot_function_2(f3, history)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eg2B9eVkjBJh"
      },
      "id": "eg2B9eVkjBJh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "1. Try with different iterations, learning rates and functions. Try non-convex functions and see how it converges!\n",
        "2. Does changing these values give you better minima estimation?"
      ],
      "metadata": {
        "id": "joeib4vUk27k"
      },
      "id": "joeib4vUk27k"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doing it for an MLP!\n",
        "\n",
        "Let us try to apply the gradient descent algorithm to a multi layer perceptron. The steps are the same. The increased complexity of the function doesn't matter as Autograd calculates the gradient for us.\n",
        "\n",
        "\n",
        "For a neural network, there are two inputs to the function: f(x, w) where x is the input (image, text etc. ) and w is the weight. The neural network will have a loss function: L(f(x, w)) or L(x, w). We will be doing gradient descent on w using dL/dw and not on x, as we can't control the input.\n",
        "\n",
        "Instead of using a single learning rate and updating the weights ourselves, Pytorch provides 'optimizers' which implement many algorithmic tricks, like variable learning rates, momentum, weight decay etc. When running gradient descent for an MLP, it is better to use an optimizer instead of manually changing the weights.\n",
        "\n",
        "\n",
        "The following code is taken from the lab on Day 1"
      ],
      "metadata": {
        "id": "XWdYnGxR7YmY"
      },
      "id": "XWdYnGxR7YmY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining a simple convolutional model"
      ],
      "metadata": {
        "id": "r6Bd2IkQ8Qtv"
      },
      "id": "r6Bd2IkQ8Qtv"
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN with 2 CONV layers and 3 FC layers\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        # output layer 10 classes\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # flatten all dimensions except batch\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "612KEadH8AOJ"
      },
      "id": "612KEadH8AOJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Take care of the data"
      ],
      "metadata": {
        "id": "8WxKg6U08VF-"
      },
      "id": "8WxKg6U08VF-"
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR10\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "zJWSu_3Z8UVK"
      },
      "id": "zJWSu_3Z8UVK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training data shape : ', trainset.data.shape, len(trainset.targets))\n",
        "print('Testing data shape : ', testset.data.shape, len(testset.targets))\n",
        "\n",
        "# Find the unique numbers from the train labels\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)"
      ],
      "metadata": {
        "id": "kiBbDo_s8zu9"
      },
      "id": "kiBbDo_s8zu9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us write a training function to implement gradient descent:"
      ],
      "metadata": {
        "id": "sUjEmRRD8pTy"
      },
      "id": "sUjEmRRD8pTy"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      # Average loss and acc values\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ],
      "metadata": {
        "id": "pzJ36IJv8xCG"
      },
      "id": "pzJ36IJv8xCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test function"
      ],
      "metadata": {
        "id": "UckvNxnA9cQg"
      },
      "id": "UckvNxnA9cQg"
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # Calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # The class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network: %d %%' % (\n",
        "      100 * correct / total))"
      ],
      "metadata": {
        "id": "CP-7OlFY9bRe"
      },
      "id": "CP-7OlFY9bRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do the training!"
      ],
      "metadata": {
        "id": "Si745mfN9kF-"
      },
      "id": "Si745mfN9kF-"
    },
    {
      "cell_type": "code",
      "source": [
        "# start with a fresh model\n",
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "gPAVUvS39g4F"
      },
      "id": "gPAVUvS39g4F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a single value to do backward() on. Thus, we need to use the appropriate loss. The common loss function for classification is cross entropy loss."
      ],
      "metadata": {
        "id": "TkSPEwL69v9t"
      },
      "id": "TkSPEwL69v9t"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Rr-A-xgf9rqZ"
      },
      "id": "Rr-A-xgf9rqZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us pick one of the many optimizers available in Pytorch. Here we are using SGD. You can substitute with any optimizer from this page: https://pytorch.org/docs/stable/optim.html#per-parameter-options"
      ],
      "metadata": {
        "id": "a9tBLICa98WB"
      },
      "id": "a9tBLICa98WB"
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD optimizer with momentum\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05, momentum=0.9)"
      ],
      "metadata": {
        "id": "Lz9tcTbq-KLv"
      },
      "id": "Lz9tcTbq-KLv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the training code:"
      ],
      "metadata": {
        "id": "8FRMq4Ju-TKQ"
      },
      "id": "8FRMq4Ju-TKQ"
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5  # iterations\n",
        "train_losses, train_acc = train(num_epochs, model, trainloader, criterion, optimizer)"
      ],
      "metadata": {
        "id": "DMAdPkMC-Vmx"
      },
      "id": "DMAdPkMC-Vmx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(train_losses)+1),train_losses)\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Loss vs Epochs')\n",
        "ax = fig.add_subplot(1,2, 2)\n",
        "ax.plot(np.arange(1,len(train_acc)+1),train_acc)\n",
        "plt.xlabel('Training accuracy')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Accuracy vs Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sDn37VzD-fN2"
      },
      "id": "sDn37VzD-fN2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy on test data after training\n",
        "test_model(model, testloader)"
      ],
      "metadata": {
        "id": "lYr4o5ud-gMJ"
      },
      "id": "lYr4o5ud-gMJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "1. Try with different hyperparameters of learning rate and momentum\n",
        "2. Try with different optimizer values."
      ],
      "metadata": {
        "id": "Oabb0UFv-t3s"
      },
      "id": "Oabb0UFv-t3s"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AikFvg7Y-z6i"
      },
      "id": "AikFvg7Y-z6i",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:experimental]",
      "language": "python",
      "name": "conda-env-experimental-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}