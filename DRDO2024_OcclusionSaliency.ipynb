{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatasl/AIML_TRAINING_VENKAT/blob/main/DRDO2024_OcclusionSaliency.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saliency maps using occlusion\n",
        "\n",
        "In this notebook, we will use the simple concept of occlusion maps to find the salient parts of an input image.\n",
        "\n",
        "An occlusion map is a map of the confidence of a model when different parts of an image are occluded.\n",
        "\n",
        "First, let us load the model we want to use"
      ],
      "metadata": {
        "id": "DZ9Iv1mvvJdt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I8HSRZSu_dC"
      },
      "outputs": [],
      "source": [
        "# import everything\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get imagenet classes\n",
        "response = requests.get('https://raw.githubusercontent.com/pytorch/hub/refs/heads/master/imagenet_classes.txt')\n",
        "text = response.text\n",
        "ImageNetClasses = text.splitlines()"
      ],
      "metadata": {
        "id": "SL8EeJKZzwQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained model\n",
        "model = models.resnet50(pretrained=True)\n",
        "# model = models.vgg16(pretrained=True)\n",
        "# go to https://pytorch.org/serve/model_zoo.html to find the names of more pretrained models\n",
        "\n",
        "# put the model on the gpu\n",
        "model.cuda()\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "jYXqpwqaxiau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the image transformation. This is important because this is how the model was trained\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    # The below values are based on the mean and st.deviation of the ImageNet dataset\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# inverse of the transform\n",
        "invert = transforms.Compose([\n",
        "        transforms.Normalize(\n",
        "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "    std=[1/0.229, 1/0.224, 1/0.255]),\n",
        "        transforms.ToPILImage()])"
      ],
      "metadata": {
        "id": "sF4MJgzGxkgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess the image\n",
        "def load_image(image_path, display=False):\n",
        "    if image_path.startswith('http'):\n",
        "        response = requests.get(image_path)\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "    else:\n",
        "        img = Image.open(image_path)\n",
        "    img = preprocess(img)\n",
        "    if display:\n",
        "      plt.imshow(invert(img))\n",
        "\n",
        "    # img = img.unsqueeze(0)  # Add batch dimension\n",
        "    return img"
      ],
      "metadata": {
        "id": "vuCQAMGsvI--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us load an image\n",
        "# image_path = 'https://raw.githubusercontent.com/pytorch/serve/refs/heads/master/examples/image_classifier/kitten.jpg'  # Replace with your image path or URL\n",
        "image_path = 'https://www.pixelstalk.net/wp-content/uploads/2016/03/Animals-baby-cat-dog-HD-wallpaper.jpg'\n",
        "img = load_image(image_path, display=True)\n"
      ],
      "metadata": {
        "id": "lXPWdbFxysxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let us do inference on the image\n",
        "with torch.no_grad():\n",
        "  outputs = model(img.unsqueeze(0).cuda())\n",
        "\n",
        "sorted = torch.argsort(-outputs.squeeze())\n",
        "\n",
        "print('The top 10 predictions are: ')\n",
        "for i in range(20):\n",
        "  print(f'{sorted[i]}: {ImageNetClasses[sorted[i]]}')\n",
        "\n",
        "_, prediction = outputs.max(1)\n",
        "print(f'Predicted class: {prediction}: {ImageNetClasses[prediction]}')\n"
      ],
      "metadata": {
        "id": "Y7qkOArO2MqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use occlusion maps, we need to create images where one patch is blacked out. I am using this Dataset class to do this:"
      ],
      "metadata": {
        "id": "nCl235EE4UYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OcclusionDataset(Dataset):\n",
        "  def __init__(self, img, window=10, stride = 5):\n",
        "    self.masterimage = img\n",
        "    self.window = window\n",
        "    self.stride = stride\n",
        "    self.pos = math.floor(224/self.stride)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.pos*self.pos\n",
        "\n",
        "  def display(self, index):\n",
        "    img, mask = self[index]\n",
        "    img = invert(img.cpu())\n",
        "    fig, axes = plt.subplots(1,2,figsize = (4, 2))\n",
        "    axes[0].imshow(img)\n",
        "    axes[0].axis('off')\n",
        "    axes[1].imshow(mask)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img = self.masterimage.clone()\n",
        "    mask = torch.ones([224,224]) # a tensor of all ones same size as image\n",
        "    row = math.floor(idx/self.pos)*self.stride\n",
        "    col = (idx%self.pos) * self.stride\n",
        "    # set a window in the mask to zero\n",
        "    mask[ row:min(row+self.window, 223), col:(min(col+self.window, 223))] = 0\n",
        "    # multiply r,g,b channels with the mask:\n",
        "    for i in range(3):\n",
        "      img[i,:,:] = img[i,:,:]*mask\n",
        "    return img.cuda(), (mask-1)*(-1)"
      ],
      "metadata": {
        "id": "txbuUzAe2dAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let us see how this class works by giving very large window and stride\n",
        "dataset = OcclusionDataset(img, window = 100, stride=100)\n",
        "for i in range(len(dataset)):\n",
        "  dataset.display(i)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ARXxhfbj4QML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us write a function to get the images from the dataset and update the confidence onto a heatmap"
      ],
      "metadata": {
        "id": "1JUZP5QRE6mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CalculateOcclusionMap(img, window=10, stride=5, pclass = -1):\n",
        "  with torch.no_grad():\n",
        "    outputs = model(img.unsqueeze(0).cuda())\n",
        "  pvalue, pred = outputs.max(1)\n",
        "  if pclass==-1:\n",
        "    pclass = pred # -1 means default. So I will set it to its predicted class\n",
        "\n",
        "  # create an all-zero array to accumulate the heatmap values\n",
        "  heatmap = torch.zeros([224,224], dtype=torch.float).cuda()\n",
        "\n",
        "  dataset =  OcclusionDataset(img, window = window, stride=stride)\n",
        "  dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "  numdata = 0\n",
        "  with torch.no_grad():\n",
        "    for ii, (oimg, mask) in enumerate(dataloader):\n",
        "      outputs = model(oimg.cuda())\n",
        "      outputs = torch.squeeze(pvalue - outputs[:,pclass])\n",
        "      heatmap = heatmap + (outputs*mask.permute(1,2,0).cuda()).sum(dim=2)\n",
        "      numdata = numdata + outputs.shape[0]\n",
        "      print(f'\\r Done {numdata} of {len(dataset)}', end='     ')\n",
        "  return heatmap.cpu()"
      ],
      "metadata": {
        "id": "WKskYHLi7O_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us find the saliency map for the default class:"
      ],
      "metadata": {
        "id": "ooRlMWRxn6om"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap = CalculateOcclusionMap(img, window=100,stride=10)"
      ],
      "metadata": {
        "id": "AffqbSBh9vZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now view the saliency map!"
      ],
      "metadata": {
        "id": "JT68KLr6f7cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(invert(img))\n",
        "# normalize heatmap for displaying\n",
        "heatmap = (heatmap - heatmap.min())/(heatmap.max()-heatmap.min())\n",
        "plt.imshow(heatmap, cmap='jet', alpha=heatmap)"
      ],
      "metadata": {
        "id": "ZFlRh4KlIdzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us find the saliency map for class 209: Chesapeake Bay retriever"
      ],
      "metadata": {
        "id": "71hCBQwkoA_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heatmap = CalculateOcclusionMap(img, window=100,stride=5, pclass=209)\n",
        "plt.imshow(invert(img))\n",
        "# normalize heatmap for displaying\n",
        "heatmap = (heatmap - heatmap.min())/(heatmap.max()-heatmap.min())\n",
        "plt.imshow(heatmap, cmap='jet', alpha=heatmap)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CKJCRIYMI07C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercises\n",
        "\n",
        "1. Try with your own images!\n",
        "2. What happens when you change the window or stride?\n",
        "3. What happens when the window is too small or too large? What would be the ideal size of the window?\n",
        "4. In images with multiple objects, try to change the class and calculate the saliency map\n",
        "5. Try to change the model. Choose another model from the modelzoo. Do you get the same saliency maps?"
      ],
      "metadata": {
        "id": "4E3QBMemocYF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eT3hvOL0o67t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}